# vLLM Configuration
# Copy this file to .env and customize the values for your setup

# Base configuration path for vLLM cache and models
CONFIG_PATH=/opt/vllm

# Model configuration
VLLM_MODEL=microsoft/DialoGPT-medium
# Alternative models:
# VLLM_MODEL=microsoft/DialoGPT-large
# VLLM_MODEL=meta-llama/Llama-2-7b-chat-hf
# VLLM_MODEL=mistralai/Mistral-7B-Instruct-v0.1

# Server configuration
VLLM_HOST=0.0.0.0
VLLM_PORT=8000

# GPU and Performance settings
VLLM_GPU_MEMORY_UTILIZATION=0.9
VLLM_MAX_MODEL_LEN=2048
VLLM_TENSOR_PARALLEL_SIZE=1

# Optional: Additional vLLM arguments
# VLLM_TRUST_REMOTE_CODE=false
# VLLM_DOWNLOAD_DIR=/root/.cache/huggingface
# VLLM_SWAP_SPACE=4
# VLLM_MAX_NUM_SEQS=256